{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import string\n",
    "import tensorflow\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers.merge import add\n",
    "from keras.models import Model,load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical,plot_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.applications.vgg16 import VGG16,preprocess_input\n",
    "from keras.layers import Input,Dense,LSTM,Embedding,Dropout\n",
    "from keras.preprocessing.image import img_to_array,load_img\n",
    "from nltk.translate.bleu_score import sentence_bleu,corpus_bleu\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(directory):\n",
    "  model = VGG16()\n",
    "  model.layers.pop()\n",
    "  model = Model(inputs=model.inputs,outputs=model.layers[-1].output)\n",
    "  print(model.summary())\n",
    "  features = {}\n",
    "  i=0\n",
    "  for name in os.listdir(directory):\n",
    "    print(i)\n",
    "    img = load_img(directory+'/'+name,target_size=(224,224))\n",
    "    img = img_to_array(img)\n",
    "    img = img.reshape((1,img.shape[0],img.shape[1],img.shape[2]))\n",
    "    img = preprocess_input(img)\n",
    "    feature = model.predict(img,verbose=0)\n",
    "    img_id = name.split('.')[0]\n",
    "    features[img_id] = feature\n",
    "    i+=1\n",
    "  return features\n",
    "directory ='drive/My Drive/image_captioning/Flicker8k/Flicker8k_Dataset'\n",
    "features = extract_features(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_description(filename):\n",
    "  mappings = {}\n",
    "  file = open(filename,'r')\n",
    "  content = file.readlines()\n",
    "  file.close()\n",
    "  for lines in content:\n",
    "    tokens = lines.split()\n",
    "    if len(lines)<2:\n",
    "      continue\n",
    "    image_id,image_desc = tokens[0].split('.')[0],tokens[1:]\n",
    "    image_desc = ' '.join(image_desc)\n",
    "    if image_id not in mappings:\n",
    "      mappings[image_id] = []\n",
    "    mappings[image_id].append(image_desc)\n",
    "  return mappings\n",
    "\n",
    "\n",
    "def clean_description(descriptions):\n",
    "  table = str.maketrans('','',string.punctuation)\n",
    "  for k,image_descriptions in descriptions.items():\n",
    "    for i in range(len(image_descriptions)):\n",
    "      desc = image_descriptions[i]\n",
    "      desc = desc.split()\n",
    "      desc = [x.lower() for x in desc]\n",
    "      desc = [w.translate(table) for w in desc]\n",
    "      desc = [x for x in desc if len(x)>1]\n",
    "      desc = [x for x in desc if x.isalpha()]\n",
    "      image_descriptions[i] = ' '.join(desc)\n",
    "\n",
    "def create_corpus(descriptions):\n",
    "  corpus = set()\n",
    "  for k in descriptions.keys():\n",
    "    [corpus.update(x.split()) for x in descriptions[k]]\n",
    "  return corpus\n",
    "\n",
    "def save_descriptions(desc,filename):\n",
    "  lines = []\n",
    "  for k,v in desc.items():\n",
    "    for description in v:\n",
    "      lines.append(k+' '+description)\n",
    "  data = '\\n'.join(lines)\n",
    "  file = open(filename,'w')\n",
    "  file.write(data)\n",
    "  file.close()\n",
    "\n",
    "# load all descriptions\n",
    "filename = 'drive/My Drive/image_captioning/Flicker8k/Flickr8k.token.txt'\n",
    "descriptions = load_description(filename)\n",
    "print('Descriptions loaded: ',len(descriptions))\n",
    "\n",
    "# clean the loaded descriptions\n",
    "clean_description(descriptions)\n",
    "\n",
    "# check the vocabulary length\n",
    "vocabulary = create_corpus(descriptions)\n",
    "print('Vocabulary length: ',len(vocabulary))\n",
    "save_descriptions(descriptions,'drive/My Drive/image_captioning/descriptions.txt')\n",
    "\n",
    "print('SAVED !!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_set_of_image_ids(filename):\n",
    "  file = open(filename,'r')\n",
    "  lines = file.readlines()\n",
    "  file.close()\n",
    "  image_ids = set()\n",
    "  for line in lines:\n",
    "    if len(line)<1:\n",
    "      continue\n",
    "    image_ids.add(line.split('.')[0])\n",
    "  return image_ids\n",
    "\n",
    "def load_clean_descriptions(all_desc,train_desc_names):\n",
    "  file = open(all_desc,'r')\n",
    "  lines = file.readlines()\n",
    "  descriptions = {}\n",
    "  for line in lines:\n",
    "    tokens = line.split()\n",
    "    image_id,image_desc = tokens[0].split('.')[0],tokens[1:]\n",
    "    if image_id in train_desc_names:\n",
    "      if image_id not in descriptions:\n",
    "        descriptions[image_id] = []\n",
    "      desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "      descriptions[image_id].append(desc)\n",
    "  return descriptions\n",
    "\n",
    "def load_image_features(filename,dataset):\n",
    "  all_features = pickle.load(open(filename,'rb'))\n",
    "  features = {k:all_features[k] for k in dataset}\n",
    "  return features\n",
    "\n",
    "# load train image ids\n",
    "train = 'drive/My Drive/image_captioning/Flicker8k/Flickr_8k.trainImages.txt'\n",
    "train_image_ids = load_set_of_image_ids(train)\n",
    "print('Training images found: ',len(train_image_ids))\n",
    "\n",
    "# load training descriptions\n",
    "train_descriptions = load_clean_descriptions('drive/My Drive/image_captioning/descriptions.txt',train_image_ids)\n",
    "print('training descriptions loaded: ',len(train_descriptions))\n",
    "\n",
    "# load training image features\n",
    "train_features = load_image_features('drive/My Drive/image_captioning/Flicker_dataset_image_features.pkl',train_image_ids)\n",
    "print('training features loaded: ',len(train_features))\n",
    "\n",
    "train_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_list(descriptions):\n",
    "  all_desc_list = []\n",
    "  for k,v in descriptions.items():\n",
    "    for desc in v:\n",
    "      all_desc_list.append(desc)\n",
    "  return all_desc_list\n",
    "\n",
    "def tokenization(descriptions):\n",
    "  # list of all the descriptions\n",
    "  all_desc_list = to_list(descriptions)  \n",
    "  tokenizer = Tokenizer()\n",
    "  tokenizer.fit_on_texts(all_desc_list)\n",
    "  return tokenizer\n",
    "\n",
    "# create tokenizer\n",
    "tokenizer = tokenization(train_descriptions)\n",
    "\n",
    "# word index is the dictionary /mappings of word-->integer\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "print('Vocab size: ',vocab_size)\n",
    "\n",
    "def max_length(descriptions):\n",
    "  all_desc_list = to_list(descriptions)\n",
    "  return (max(len(x.split()) for x in all_desc_list))\n",
    "\n",
    "\n",
    "def create_sequences(tokenizer,desc_list,max_len,photo):\n",
    "  X1,X2,y = [],[],[]\n",
    "  # X1 will contain photo\n",
    "  # X2 will contain current sequence\n",
    "  # y will contain one hot encoded next word\n",
    "\n",
    "  for desc in desc_list:\n",
    "    # tokenize descriptions\n",
    "    seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "    for i in range(1,len(seq)):\n",
    "      # out seq is basically the next word in the sentence\n",
    "      in_seq,out_seq = seq[:i],seq[i]\n",
    "      # pad input sequence\n",
    "      in_seq = pad_sequences([in_seq],maxlen=max_len)[0]\n",
    "      # one hot encode output sequence\n",
    "      out_seq = to_categorical([out_seq],num_classes=vocab_size)[0]\n",
    "      X1.append(photo)\n",
    "      X2.append(in_seq)\n",
    "      y.append(out_seq)\n",
    "  return np.array(X1),np.array(X2),np.array(y)\n",
    "\n",
    "# maximum length that a description can have OR the biggest description we are having\n",
    "max_len = max_length(train_descriptions)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(descriptions,photos,tokenizer,max_len):\n",
    "  while 1:\n",
    "    for k,desc_list in descriptions.items():\n",
    "      photo = photos[k][0]\n",
    "      in_img,in_seq,out_seq = create_sequences(tokenizer,desc_list,max_len,photo)\n",
    "      yield[[in_img,in_seq],out_seq]\n",
    "\n",
    "def define_model(vocab_size, max_length):\n",
    "    # image features extractor model\n",
    "    inputs1 = Input(shape=(4096,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    " \n",
    "    # input sequence model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "     # embedding(input_dimension,output_dimension,)\n",
    "     # input dim is always the vocabulary size \n",
    "    # output dimension tells the size of vector space in which the words will be embedded\n",
    "    # mask zero is used when the input itself is 0 then to not confuse it with padded zeros it is used as True\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "\n",
    "    # decoder model OR output word model\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    # summarize model\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_model(vocab_size,max_len)\n",
    "epochs = 20\n",
    "steps = len(train_descriptions)\n",
    "for i in range(epochs):\n",
    "  generator = data_generator(train_descriptions,train_features,tokenizer,max_len)\n",
    "  model.fit_generator(generator,epochs=1,steps_per_epoch=steps,verbose=1)\n",
    "  model.save('drive/My Drive/image_captioning/model_'+str(i)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int2word(tokenizer,integer):\n",
    "  for word,index in tokenizer.word_index.items():\n",
    "    if index==integer:\n",
    "      return word\n",
    "  return None\n",
    "\n",
    "def predict_desc(model,tokenizer,photo,max_len):\n",
    "  in_seq = 'startseq'\n",
    "  for i in range(max_len):\n",
    "    seq = tokenizer.texts_to_sequences([in_seq])[0]\n",
    "    seq = pad_sequences([seq],maxlen=max_len)\n",
    "    y_hat = model.predict([photo,seq],verbose=0)\n",
    "    y_hat = np.argmax(y_hat)\n",
    "    word = int2word(tokenizer,y_hat)\n",
    "    if word==None:\n",
    "      break\n",
    "    in_seq = in_seq+' '+word\n",
    "    if word=='endseq':\n",
    "      break\n",
    "  return in_seq\n",
    "\n",
    "def evaluate_model(model,descriptions,photos,tokenizer,max_len):\n",
    "  actual,predicted = [],[]\n",
    "  for key,desc in descriptions.items():\n",
    "    y_hat = predict_desc(model,tokenizer,photos[key],max_len)\n",
    "    references = [d.split() for d in desc]\n",
    "    actual.append(references)\n",
    "    predicted.append(y_hat.split())\n",
    "  print('BLEU-1: %f' %corpus_bleu(actual,predicted,weights=(1.0,0,0,0)))\n",
    "  print('BLEU-2: %f' %corpus_bleu(actual,predicted,weights=(0.5,0.5,0,0)))\n",
    "  print('BLEU-3: %f' %corpus_bleu(actual,predicted,weights=(0.33,0.33,0.33,0)))\n",
    "  print('BLEU-4: %f' %corpus_bleu(actual,predicted,weights=(0.25,0.25,0.25,0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################  load training data (6k)  ##########################\n",
    "train = 'drive/My Drive/image_captioning/Flicker8k/Flickr_8k.trainImages.txt'\n",
    "train_image_ids = load_set_of_image_ids(train)\n",
    "print('Training images found: ',len(train_image_ids))\n",
    "\n",
    "# load training descriptions\n",
    "train_descriptions = load_clean_descriptions('drive/My Drive/image_captioning/descriptions.txt',train_image_ids)\n",
    "print('training descriptions loaded: ',len(train_descriptions))\n",
    "\n",
    "tokenizer = tokenization(train_descriptions)\n",
    "max_len = max_length(train_descriptions)\n",
    "\n",
    "####################  load test data  ##########################\n",
    "test = 'drive/My Drive/image_captioning/Flicker8k/Flickr_8k.testImages.txt'\n",
    "test_image_ids = load_set_of_image_ids(test)\n",
    "print('Test images found: ',len(test_image_ids))\n",
    "\n",
    "# load test descriptions\n",
    "test_descriptions = load_clean_descriptions('drive/My Drive/image_captioning/descriptions.txt',test_image_ids)\n",
    "print('test descriptions loaded: ',len(test_descriptions))\n",
    "\n",
    "# load test image features\n",
    "test_features = load_image_features('drive/My Drive/image_captioning/Flicker_dataset_image_features.pkl',test_image_ids)\n",
    "print('training features loaded: ',len(test_features))\n",
    "#################################################################\n",
    "filename = 'drive/My Drive/image_captioning/model_18.h5'\n",
    "model = load_model(filename)\n",
    "evaluate_model(model,test_descriptions,test_features,tokenizer,max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_to_test = 'drive/My Drive/image_captioning/983801190.jpg'\n",
    "img = plt.imread(img_to_test)\n",
    "plt.imshow(img)\n",
    "\n",
    "def extract_features(filename):\n",
    "    # load the model\n",
    "    model = VGG16()\n",
    "    # re-structure the model\n",
    "    model.layers.pop()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    # load the photo\n",
    "    image = load_img(filename, target_size=(224, 224))\n",
    "    # convert the image pixels to a numpy array\n",
    "    image = img_to_array(image)\n",
    "    # reshape data for the model\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    # prepare the image for the VGG model\n",
    "    image = preprocess_input(image)\n",
    "    # get features\n",
    "    feature = model.predict(image, verbose=0)\n",
    "    return feature\n",
    "\n",
    "# pre-define the max sequence length (from training)\n",
    "max_length = 34\n",
    "# load the model\n",
    "model = load_model('drive/My Drive/image_captioning/model_18.h5')\n",
    "# load and prepare the photograph\n",
    "photo = extract_features(img_to_test)\n",
    "# generate description\n",
    "description = predict_desc(model, tokenizer, photo, max_length)\n",
    "\n",
    "description = ' '.join(description.split()[1:-1])\n",
    "print()\n",
    "print(description)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "876db7d3f8682b7e4c5477e5f1e2dc2bd1b19fe05d9aabd59218369070754b39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
